{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BirdCLEF 2025 Inference Notebook**\n",
    "This notebook runs inference on BirdCLEF 2025 test soundscapes and generates a submission file. It supports both single model inference and ensemble inference with multiple models. You can find the pre-processing and training processes in the following notebooks:\n",
    "\n",
    "- [Transforming Audio-to-Mel Spec. | BirdCLEF'25](https://www.kaggle.com/code/kadircandrisolu/transforming-audio-to-mel-spec-birdclef-25)  \n",
    "- [EfficientNet B0 Pytorch [Train] | BirdCLEF'25](https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-train-birdclef-25)\n",
    "\n",
    "**Features**\n",
    "- Audio Preprocessing\n",
    "- Test-Time Augmentation (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.150784Z",
     "iopub.status.busy": "2025-05-11T14:30:43.150420Z",
     "iopub.status.idle": "2025-05-11T14:30:43.156280Z",
     "shell.execute_reply": "2025-05-11T14:30:43.155331Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.150759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.158125Z",
     "iopub.status.busy": "2025-05-11T14:30:43.157782Z",
     "iopub.status.idle": "2025-05-11T14:30:43.172373Z",
     "shell.execute_reply": "2025-05-11T14:30:43.171536Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.158105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    " \n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_path = '/kaggle/input/birdclef25-efficientnet-pseudlabled/pytorch/primary-only-thr-0.9/3'  \n",
    "    \n",
    "    # Audio parameters\n",
    "    FS = 32000  \n",
    "    WINDOW_SIZE = 5  \n",
    "    TARGET_DURATION = 5.0\n",
    "    \n",
    "    # Mel spectrogram parameters\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 14000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "    \n",
    "    model_name = 'efficientnet_b0'\n",
    "    in_channels = 1\n",
    "    device = 'cpu'\n",
    "    pretrained = False\n",
    "    \n",
    "    # Inference parameters\n",
    "    batch_size = 16\n",
    "    use_tta = False  \n",
    "    tta_count = 3\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Features for compatibility with training notebook\n",
    "    dropout_rate = 0.2\n",
    "    drop_path_rate = 0.2\n",
    "    \n",
    "    # Add parameters to ensure compatibility with training model\n",
    "    projection_dim = 0  # Set to 0 to match training model default\n",
    "    \n",
    "    use_specific_folds = False  # If False, use all found models\n",
    "    folds = [0, 1]  # Used only if use_specific_folds is True\n",
    "    \n",
    "    debug = False\n",
    "    debug_count = 3\n",
    "    \n",
    "    # Debug option for state dict loading\n",
    "    debug_state_dict = False  # Set to True to print missing keys\n",
    "\n",
    "cfg = CFG()\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.173714Z",
     "iopub.status.busy": "2025-05-11T14:30:43.173285Z",
     "iopub.status.idle": "2025-05-11T14:30:43.197604Z",
     "shell.execute_reply": "2025-05-11T14:30:43.196836Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.173689Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.199365Z",
     "iopub.status.busy": "2025-05-11T14:30:43.199125Z",
     "iopub.status.idle": "2025-05-11T14:30:43.210433Z",
     "shell.execute_reply": "2025-05-11T14:30:43.209662Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.199347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Support for different model architectures\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=cfg.dropout_rate,\n",
    "            drop_path_rate=cfg.drop_path_rate if hasattr(cfg, 'drop_path_rate') else 0.2\n",
    "        )\n",
    "        \n",
    "        # Extract feature dimension based on model type\n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'convnext' in cfg.model_name:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.feat_dim = backbone_out\n",
    "        \n",
    "        # Add an additional projection layer for better feature representation\n",
    "        if hasattr(cfg, 'projection_dim') and cfg.projection_dim > 0:\n",
    "            self.projection = nn.Sequential(\n",
    "                nn.Linear(backbone_out, cfg.projection_dim),\n",
    "                nn.BatchNorm1d(cfg.projection_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(cfg.projection_dim, num_classes)\n",
    "            )\n",
    "            self.classifier = self.projection\n",
    "        else:\n",
    "            self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "        \n",
    "        # Mixup and CutMix support\n",
    "        self.mixup_enabled = False\n",
    "        self.cutmix_enabled = False\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "            \n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "        \n",
    "    def mixup_data(self, x, targets):\n",
    "        \"\"\"Applies mixup to the data batch\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "        indices = torch.randperm(batch_size).to(x.device, non_blocking=True)\n",
    "        mixed_x = lam * x + (1 - lam) * x[indices]\n",
    "        \n",
    "        return mixed_x, targets, targets[indices], lam\n",
    "        \n",
    "    def cutmix_data(self, x, targets):\n",
    "        \"\"\"Applies cutmix to the data batch\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        lam = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n",
    "        \n",
    "        # Get random indices for mixing\n",
    "        indices = torch.randperm(batch_size).to(x.device)\n",
    "        \n",
    "        # Get random box coordinates\n",
    "        W, H = x.size(2), x.size(3)\n",
    "        cut_ratio = np.sqrt(1. - lam)\n",
    "        cut_w = np.int_(W * cut_ratio)\n",
    "        cut_h = np.int_(H * cut_ratio)\n",
    "        \n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "        \n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "        \n",
    "        # Apply cutmix\n",
    "        x_mixed = x.clone()\n",
    "        x_mixed[:, :, bbx1:bbx2, bby1:bby2] = x[indices, :, bbx1:bbx2, bby1:bby2]\n",
    "        \n",
    "        # Adjust lambda to actual area ratio\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "        \n",
    "        return x_mixed, targets, targets[indices], lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.232374Z",
     "iopub.status.busy": "2025-05-11T14:30:43.232063Z",
     "iopub.status.idle": "2025-05-11T14:30:43.251439Z",
     "shell.execute_reply": "2025-05-11T14:30:43.250634Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.232349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data, \n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "                          mode='constant')\n",
    "    \n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    return mel_spec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.252842Z",
     "iopub.status.busy": "2025-05-11T14:30:43.252456Z",
     "iopub.status.idle": "2025-05-11T14:30:43.270181Z",
     "shell.execute_reply": "2025-05-11T14:30:43.269282Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.252816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "    \n",
    "    model_dir = Path(cfg.model_path)\n",
    "    \n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    model_files = find_model_files(cfg)\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    if cfg.use_specific_folds:\n",
    "        filtered_files = []\n",
    "        for fold in cfg.folds:\n",
    "            fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "            filtered_files.extend(fold_files)\n",
    "        model_files = filtered_files\n",
    "        print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n",
    "    \n",
    "    for model_path in model_files:\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=cfg.device)\n",
    "            \n",
    "            model = BirdCLEFModel(cfg, num_classes)\n",
    "            \n",
    "            # Handle different state dict configurations\n",
    "            state_dict = None\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['model_state_dict']\n",
    "            elif 'state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['state_dict']\n",
    "            else:\n",
    "                # Assume the checkpoint itself is the state dict\n",
    "                state_dict = checkpoint\n",
    "            \n",
    "            # Handle DataParallel wrapped state dict (remove 'module.' prefix)\n",
    "            if any(k.startswith('module.') for k in state_dict.keys()):\n",
    "                state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "            \n",
    "            # Check for missing keys before loading\n",
    "            model_dict = model.state_dict()\n",
    "            missing_keys = [k for k in model_dict.keys() if k not in state_dict]\n",
    "            unexpected_keys = [k for k in state_dict.keys() if k not in model_dict]\n",
    "            \n",
    "            if missing_keys and cfg.debug_state_dict:\n",
    "                print(f\"Missing keys: {missing_keys}\")\n",
    "            if unexpected_keys and cfg.debug_state_dict:\n",
    "                print(f\"Unexpected keys: {unexpected_keys}\")\n",
    "            \n",
    "            # Try to partially load if there are missing or unexpected keys\n",
    "            if missing_keys or unexpected_keys:\n",
    "                print(f\"Warning: {len(missing_keys)} missing keys, {len(unexpected_keys)} unexpected keys\")\n",
    "                # Filter state_dict to only include keys that are in the model\n",
    "                filtered_state_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "                model_dict.update(filtered_state_dict)\n",
    "                model.load_state_dict(model_dict, strict=False)\n",
    "                print(\"Loaded model with partial state dict\")\n",
    "            else:\n",
    "                model.load_state_dict(state_dict)\n",
    "                \n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            \n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        \n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        \n",
    "        for segment_idx in range(total_segments):\n",
    "            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "            segment_audio = audio_data[start_sample:end_sample]\n",
    "            \n",
    "            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "            row_ids.append(row_id)\n",
    "\n",
    "            if cfg.use_tta:\n",
    "                all_preds = []\n",
    "                \n",
    "                for tta_idx in range(cfg.tta_count):\n",
    "                    mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                    mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "\n",
    "                    mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                    mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "                    if len(models) == 1:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = models[0](mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            all_preds.append(probs)\n",
    "                    else:\n",
    "                        segment_preds = []\n",
    "                        for model in models:\n",
    "                            with torch.no_grad():\n",
    "                                outputs = model(mel_spec)\n",
    "                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                                segment_preds.append(probs)\n",
    "                        \n",
    "                        avg_preds = np.mean(segment_preds, axis=0)\n",
    "                        all_preds.append(avg_preds)\n",
    "\n",
    "                final_preds = np.mean(all_preds, axis=0)\n",
    "            else:\n",
    "                mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                \n",
    "                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                mel_spec = mel_spec.to(cfg.device)\n",
    "                \n",
    "                if len(models) == 1:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = models[0](mel_spec)\n",
    "                        final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    segment_preds = []\n",
    "                    for model in models:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            segment_preds.append(probs)\n",
    "\n",
    "                    final_preds = np.mean(segment_preds, axis=0)\n",
    "                    \n",
    "            predictions.append(final_preds)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.271930Z",
     "iopub.status.busy": "2025-05-11T14:30:43.271306Z",
     "iopub.status.idle": "2025-05-11T14:30:43.290954Z",
     "shell.execute_reply": "2025-05-11T14:30:43.290094Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.271884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_tta(spec, tta_idx):\n",
    "    \"\"\"Apply test-time augmentation\"\"\"\n",
    "    if tta_idx == 0:\n",
    "        # Original spectrogram\n",
    "        return spec\n",
    "    elif tta_idx == 1:\n",
    "        # Time shift (horizontal flip)\n",
    "        return np.flip(spec, axis=1)\n",
    "    elif tta_idx == 2:\n",
    "        # Frequency shift (vertical flip)\n",
    "        return np.flip(spec, axis=0)\n",
    "    else:\n",
    "        return spec\n",
    "\n",
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.293370Z",
     "iopub.status.busy": "2025-05-11T14:30:43.293138Z",
     "iopub.status.idle": "2025-05-11T14:30:43.310018Z",
     "shell.execute_reply": "2025-05-11T14:30:43.309271Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.293353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference...\")\n",
    "    print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "    print(f\"Using model architecture: {cfg.model_name}\")\n",
    "\n",
    "    models = load_models(cfg, num_classes)\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_model_compatibility():\n",
    "    \"\"\"\n",
    "    Function to debug model compatibility issues\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Debugging Model Compatibility ===\")\n",
    "    \n",
    "    print(f\"Creating a test model with the current configuration...\")\n",
    "    test_model = BirdCLEFModel(cfg, num_classes)\n",
    "    test_state_dict = test_model.state_dict()\n",
    "    \n",
    "    print(f\"Model has {len(test_state_dict)} parameters\")\n",
    "    \n",
    "    # Display some parameter shapes to help debug\n",
    "    print(\"\\nSample parameter shapes from current model:\")\n",
    "    for i, (name, param) in enumerate(test_state_dict.items()):\n",
    "        if i < 5 or i > len(test_state_dict) - 5:  # First 5 and last 5 params\n",
    "            print(f\"{name}: {param.shape}\")\n",
    "        if i == 5 and len(test_state_dict) > 10:\n",
    "            print(\"...\")\n",
    "            \n",
    "    # Try to load sample model\n",
    "    model_files = find_model_files(cfg)\n",
    "    if model_files:\n",
    "        sample_file = model_files[0]\n",
    "        print(f\"\\nAttempting to load a sample model: {sample_file}\")\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(sample_file, map_location=cfg.device)\n",
    "            \n",
    "            # Check what's in the checkpoint\n",
    "            if isinstance(checkpoint, dict):\n",
    "                print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "                \n",
    "                # Look at state dict keys\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    state_dict = checkpoint['model_state_dict']\n",
    "                elif 'state_dict' in checkpoint:\n",
    "                    state_dict = checkpoint['state_dict']\n",
    "                else:\n",
    "                    state_dict = None\n",
    "                    print(\"No state dict found in checkpoint\")\n",
    "                \n",
    "                if state_dict:\n",
    "                    print(f\"State dict has {len(state_dict)} parameters\")\n",
    "                    \n",
    "                    # Count mismatches\n",
    "                    missing = [k for k in test_state_dict.keys() if k not in state_dict]\n",
    "                    unexpected = [k for k in state_dict.keys() if k not in test_state_dict]\n",
    "                    \n",
    "                    print(f\"Missing keys: {len(missing)}\")\n",
    "                    print(f\"Unexpected keys: {len(unexpected)}\")\n",
    "                    \n",
    "                    if missing:\n",
    "                        print(\"\\nSample missing keys:\")\n",
    "                        for k in missing[:5]:\n",
    "                            print(f\"  {k}\")\n",
    "                    \n",
    "                    if unexpected:\n",
    "                        print(\"\\nSample unexpected keys:\")\n",
    "                        for k in unexpected[:5]:\n",
    "                            print(f\"  {k}\")\n",
    "                        \n",
    "                    # Try to map keys\n",
    "                    if unexpected and missing:\n",
    "                        print(\"\\nAttempting to map keys...\")\n",
    "                        successful_maps = 0\n",
    "                        for unexp_key in unexpected[:10]:\n",
    "                            for miss_key in missing:\n",
    "                                # Simple heuristic: check if the key ends with the same parameter name\n",
    "                                if unexp_key.split('.')[-1] == miss_key.split('.')[-1]:\n",
    "                                    print(f\"Possible mapping: {unexp_key} -> {miss_key}\")\n",
    "                                    successful_maps += 1\n",
    "                                    break\n",
    "                        \n",
    "                        if successful_maps == 0:\n",
    "                            print(\"No obvious mappings found. Model architectures may be different.\")\n",
    "            else:\n",
    "                print(\"Checkpoint is not a dictionary - it may be a direct state dict\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n=== End of Compatibility Debugging ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.311088Z",
     "iopub.status.busy": "2025-05-11T14:30:43.310789Z",
     "iopub.status.idle": "2025-05-11T14:30:43.406526Z",
     "shell.execute_reply": "2025-05-11T14:30:43.405673Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.311008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "TTA enabled: False (variations: 0)\n",
      "Found a total of 1 model files.\n",
      "Loading model: /kaggle/input/birdclef25-efficientnet-pseudlabled/pytorch/primary-only-thr-0.9/3/model_20250501_131702_efficientnet_b0_fold0.pth\n",
      "Error loading model /kaggle/input/birdclef25-efficientnet-pseudlabled/pytorch/primary-only-thr-0.9/3/model_20250501_131702_efficientnet_b0_fold0.pth: BirdCLEFModel.__init__() takes 2 positional arguments but 3 were given\n",
      "No models found! Please check model paths.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Debug model compatibility if needed\n",
    "    if cfg.debug_state_dict:\n",
    "        debug_model_compatibility()\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 6891568,
     "sourceId": 11060723,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 234673081,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 320888,
     "modelInstanceId": 300331,
     "sourceId": 361214,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 320888,
     "modelInstanceId": 300331,
     "sourceId": 366989,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 324738,
     "modelInstanceId": 304257,
     "sourceId": 367012,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 320888,
     "modelInstanceId": 300331,
     "sourceId": 367914,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 320888,
     "modelInstanceId": 300331,
     "sourceId": 387055,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
