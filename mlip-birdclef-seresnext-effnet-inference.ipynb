{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11948763,"sourceType":"datasetVersion","datasetId":7130272},{"sourceId":426264,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":347484,"modelId":365373},{"sourceId":427449,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":348452,"modelId":365373}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Inference Notebook for final submission\nCombined data filtering, pseudo-labels and spectrogram augmentations in one final model. \\\nUsing more powerful [SE-ResNeXt](https://paperswithcode.com/model/seresnext?variant=seresnext50-32x4d) model together with EfficientNet. \\\nFor more informmation, see training notebook:\n- [MLiP Group 25 BirdCLEF 2025 Training Notebook](https://www.kaggle.com/code/maxgewald/mlip-25-birdclef2025-training) \n\nThis notebook is a modified version of:\n- [Bird2025 | Single SED Model Inference [LB 0.857]](www.kaggle.com/code/maxgewald/mlip-birdclef-seresnext-inference/edit)","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport warnings\nimport logging\nimport time\nimport math\nimport cv2\nfrom pathlib import Path\nimport joblib\n\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport soundfile as sf\nfrom soundfile import SoundFile \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast, GradScaler\nimport timm\nfrom tqdm.auto import tqdm\nfrom glob import glob\nimport torchaudio\nimport random\nimport itertools\nfrom typing import Union\n\nimport concurrent.futures\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.ERROR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    \n    seed = 42\n    print_freq = 100\n    num_workers = 4\n    batch_size=1\n\n    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n    model_paths = ['/kaggle/input/birdclef-2025-mlip-submission/pytorch/label-filtering-pseudo-labels-5-folds/1/', '/kaggle/input/birdclef-2025-mlip-submission/pytorch/seresnext-full-training/1']\n    folds = [[0,1,2,4],[0,1,3,4]]\n \n    pretrained = False\n    in_channels = 1\n    \n    # Mel spectrogram parameters\n    n_fft = 1024\n    hop_length = 512\n    n_mels = 128\n    f_min = 50\n    f_max = 14000\n    target_shape = (256,256)\n\n    projection_dim = 512\n    projection_dropout = 0.0\n    \n    SR = 32000\n    WINDOW_SIZE=5\n    target_duration = 5\n    infer_duration = 5\n    train_duration = 5\n    \n    device = 'cpu'\n\ncfg = CFG()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Using device: {cfg.device}\")\nprint(f\"Loading taxonomy data...\")\ntaxonomy_df = pd.read_csv(cfg.taxonomy_csv)\nspecies_ids = taxonomy_df['primary_label'].tolist()\nnum_classes = len(species_ids)\nprint(f\"Number of classes: {num_classes}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed=42):\n    \"\"\"\n    Set seed for reproducibility\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(cfg.seed)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AttBlockV2(nn.Module):\n    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n        super().__init__()\n\n        self.activation = activation\n        self.att = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True,\n        )\n        self.cla = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True,\n        )\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n\n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x, norm_att, cla\n\n    def nonlinear_transform(self, x):\n        if self.activation == \"linear\":\n            return x\n        elif self.activation == \"sigmoid\":\n            return torch.sigmoid(x)\n\n\ndef init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.0)\n\ndef init_bn(bn):\n    bn.bias.data.fill_(0.0)\n    bn.weight.data.fill_(1.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SeResNextModel(nn.Module):\n    def __init__(self, cfg, name):\n        super().__init__()\n        \n        taxonomy_df = pd.read_csv('/kaggle/input/birdclef-2025/taxonomy.csv')\n        self.num_classes = len(taxonomy_df)\n\n        self.bn0 = nn.BatchNorm2d(cfg.target_shape[0])\n        \n        self.backbone = timm.create_model(\n            name,\n            pretrained=False,\n            in_chans=cfg.in_channels,\n            drop_rate=0.2,\n            drop_path_rate=0.2,\n        )\n\n        layers = list(self.backbone.children())[:-2]\n        self.encoder = nn.Sequential(*layers)\n\n        backbone_out = self.backbone.fc.in_features\n        self.backbone.fc = nn.Identity()\n        \n        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n\n        # this is not used, but needed for compatibility\n        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=cfg.SR,\n            hop_length=cfg.hop_length,\n            n_mels=cfg.n_mels,\n            f_min=cfg.f_min,\n            f_max=cfg.f_max,\n            n_fft=cfg.n_fft,\n            pad_mode=\"constant\",\n            norm=\"slaney\",\n            onesided=True,\n            mel_scale=\"htk\",\n        )\n\n\n    def extract_feature(self,x):\n        x = x.permute((0, 1, 3, 2))\n        frames_num = x.shape[2]\n        \n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n        \n        x = x.transpose(2, 3)\n        # (batch_size, channels, freq, frames)\n        x = self.encoder(x)\n        \n        # (batch_size, channels, frames)\n        x = torch.mean(x, dim=2)\n        \n        # channel smoothing\n        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x = x1 + x2\n        \n        x = F.dropout(x, p=0.5, training=self.training)\n        x = x.transpose(1, 2)\n        x = F.relu_(self.fc1(x))\n        x = x.transpose(1, 2)\n        x = F.dropout(x, p=0.5, training=self.training)\n        return x, frames_num\n\n    def forward(self, x):\n\n        x, frames_num = self.extract_feature(x)\n        \n        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n        segmentwise_output = segmentwise_output.transpose(1, 2)\n\n        return torch.logit(clipwise_output)\n\n    def infer(self, x, tta_delta=2):\n\n        x,_ = self.extract_feature(x)\n        time_att = torch.tanh(self.att_block.att(x))\n        feat_time = x.size(-1)\n        start = (\n            feat_time / 2 - feat_time * (cfg.infer_duration / cfg.train_duration) / 2\n        )\n        end = start + feat_time * (cfg.infer_duration / cfg.train_duration)\n        start = int(start)\n        end = int(end)\n        pred = self.attention_infer(start,end,x,time_att)\n\n        start_minus = max(0, start-tta_delta)\n        end_minus=end-tta_delta\n        pred_minus = self.attention_infer(start_minus,end_minus,x,time_att)\n\n        start_plus = start+tta_delta\n        end_plus=min(feat_time, end+tta_delta)\n        pred_plus = self.attention_infer(start_plus,end_plus,x,time_att)\n\n        pred = 0.5*pred + 0.25*pred_minus + 0.25*pred_plus\n        return pred\n        \n    def attention_infer(self,start,end,x,time_att):\n        feat = x[:, :, start:end]\n\n        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n        framewise_pred_max = framewise_pred.max(dim=2)[0]\n\n        return framewise_pred_max","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EffnetModel(nn.Module):\n    def __init__(self, cfg, name):\n        super().__init__()\n        self.cfg = cfg\n        self.backbone = timm.create_model(\n            name,\n            pretrained=cfg.pretrained,\n            in_chans=cfg.in_channels,\n        )\n        backbone_out = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        \n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.feat_dim = backbone_out\n        \n        if cfg.projection_dim > 0:\n            self.projection = nn.Sequential(\n                nn.Linear(backbone_out, cfg.projection_dim),\n                nn.BatchNorm1d(cfg.projection_dim),\n                nn.ReLU(inplace=True),\n                nn.Dropout(cfg.projection_dropout),\n                nn.Linear(cfg.projection_dim, num_classes)\n            )\n            self.classifier = self.projection\n        else:\n            self.classifier = nn.Linear(backbone_out, num_classes)\n\n        \n    def forward(self, x, targets=None):\n\n        features = self.backbone(x)\n\n        if isinstance(features, dict):\n            features = features['features']\n            \n        if len(features.shape) == 4:\n            features = self.pooling(features)\n            features = features.view(features.size(0), -1)\n        \n        logits = self.classifier(features)\n        return logits\n\n    def infer(self, x):\n        return torch.sigmoid(self.forward(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transform_to_spec(audio_data, cfg):\n\n    if np.isnan(audio_data).any():\n        mean_signal = np.nanmean(audio_data)\n        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n\n    mel_spec = librosa.feature.melspectrogram(\n        y=audio_data,\n        sr=cfg.SR,\n        n_fft=cfg.n_fft,\n        hop_length=cfg.hop_length,\n        n_mels=cfg.n_mels,\n        fmin=cfg.f_min,\n        fmax=cfg.f_max,\n        power=2.0\n    )\n\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    mel_min, mel_max = mel_spec_db.min(), mel_spec_db.max()\n    mel_spec_norm = (mel_spec_db - mel_min) / (mel_max - mel_min + 1e-8)\n    \n    if mel_spec_norm.shape != cfg.target_shape:\n        mel_spec_norm = cv2.resize(mel_spec_norm, cfg.target_shape, interpolation=cv2.INTER_LINEAR)\n    \n    return torch.tensor(mel_spec_norm, dtype=torch.float32).unsqueeze(0).unsqueeze(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_sample(path, cfg):\n    audio, _ = sf.read(path, dtype=\"float32\")\n    audio_length = cfg.SR * cfg.target_duration\n    step = audio_length\n    segments = []\n    \n    # Pre-calculate all segments\n    for i in range(audio_length, len(audio) + step, step):\n        start = max(0, i - audio_length)\n        end = start + audio_length\n        if end <= len(audio):\n            segments.append((start, end))\n    \n    # Pad audio once\n    padded_audio = np.pad(audio, (len(audio), len(audio)), mode='wrap')\n    \n    audios = []\n    train_length = int(cfg.SR * cfg.train_duration)\n    target_length = int(cfg.SR * cfg.target_duration)\n    pad_length = (train_length - target_length) // 2\n    \n    for i, (start, end) in enumerate(segments):\n        center = len(audio) + (start + end) // 2\n        segment_start = center - train_length // 2\n        segment_end = segment_start + train_length\n        \n        y = padded_audio[segment_start:segment_end].astype(np.float32)\n        \n        # Apply padding only at boundaries\n        if i == 0:\n            y[:pad_length] = 0\n        elif i == len(segments) - 1:\n            y[-pad_length:] = 0\n            \n        audios.append(y)\n    \n    return audios","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_model_files(cfg):\n    \"\"\"\n    Find all .pth model files in the specified model directory\n    \"\"\"\n    model_files = []\n    \n    model_dirs = [Path(path) for path in cfg.model_paths]\n\n    for i, model_dir in enumerate(model_dirs):\n        for path in model_dir.glob('**/*.pth'):\n            for fold in cfg.folds[i]:\n                if f\"fold{fold}\" in str(path):\n                    model_files.append(str(path))\n    \n    return model_files\n\ndef load_models(cfg, num_classes):\n    \"\"\"\n    Load all found model files and prepare them for ensemble\n    \"\"\"\n    models = []\n    \n    model_files = find_model_files(cfg)\n    \n    if not model_files:\n        print(f\"Warning: No model files found under {cfg.model_paths}!\")\n        return models\n    \n    print(f\"Found a total of {len(model_files)} model files.\")\n    \n    for i, model_path in enumerate(model_files):\n        print(f\"Loading model: {model_path}\")\n        checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=False)\n    \n        if \"efficientnet_b3\" in model_path:\n            model = EffnetModel(cfg, \"efficientnet_b3\")\n        elif \"resnext\" in model_path:\n            model = SeResNextModel(cfg, \"seresnext26t_32x4d\")\n            \n        state_dict = checkpoint['model_state_dict']\n        if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n            state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n            \n        model.load_state_dict(state_dict)\n        model = model.to(cfg.device)\n        model.eval()\n        model.zero_grad()\n        model = model.half().float()\n        models.append(model)\n    \n    return models\n\ndef predict_on_spectrogram(audio_path, models, cfg, species_ids):\n    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n    predictions = []\n    row_ids = []\n    soundscape_id = Path(audio_path).stem\n    \n    try:\n        print(f\"Processing {soundscape_id}\")\n        audio_data, _ = librosa.load(audio_path, sr=cfg.SR)\n        \n        total_segments = int(len(audio_data) / (cfg.SR * cfg.WINDOW_SIZE))\n        \n        for segment_idx in range(total_segments):\n            start_sample = segment_idx * cfg.SR * cfg.WINDOW_SIZE\n            end_sample = start_sample + cfg.SR * cfg.WINDOW_SIZE\n            segment_audio = audio_data[start_sample:end_sample]\n            \n            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n            row_id = f\"{soundscape_id}_{end_time_sec}\"\n            row_ids.append(row_id)\n\n            mel_spec = transform_to_spec(segment_audio, cfg)\n            \n            if len(models) == 1:\n                with torch.no_grad():\n                    outputs = models[0].infer(mel_spec)\n                    final_preds = outputs.squeeze()\n            else:\n                segment_preds = []\n                for model in models:\n                    with torch.no_grad():\n                        outputs = model.infer(mel_spec)\n                        probs = outputs.squeeze()\n                        segment_preds.append(probs)\n\n                    final_preds = np.mean(segment_preds, axis=0)\n                    \n            predictions.append(final_preds)\n            \n    except Exception as e:\n        print(f\"Error processing {audio_path}: {e}\")\n    \n    return row_ids, predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_inference(cfg, models, species_ids):\n    \"\"\"Run inference on all test soundscapes\"\"\"\n    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n    if len(test_files) == 0:\n        test_files = sorted(glob(str(Path('/kaggle/input/birdclef-2025/train_soundscapes') / '*.ogg')))[:10]\n    \n    print(f\"Found {len(test_files)} test soundscapes\")\n\n    all_row_ids = []\n    all_predictions = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cfg.num_workers) as executor:\n        results = list(\n        executor.map(\n            predict_on_spectrogram,\n            test_files,\n            itertools.repeat(models),\n            itertools.repeat(cfg),\n            itertools.repeat(species_ids)\n        )\n    )\n\n    for rids, preds in results:\n        all_row_ids.extend(rids)\n        all_predictions.extend(preds)\n    \n    return all_row_ids, all_predictions\n\ndef create_submission(row_ids, predictions, species_ids, cfg):\n    \"\"\"Create submission dataframe\"\"\"\n    print(\"Creating submission dataframe...\")\n\n    submission_dict = {'row_id': row_ids}\n    \n    for i, species in enumerate(species_ids):\n        submission_dict[species] = [pred[i] for pred in predictions]\n\n    submission_df = pd.DataFrame(submission_dict)\n\n    submission_df.set_index('row_id', inplace=True)\n\n    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n\n    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n    if missing_cols:\n        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n        for col in missing_cols:\n            submission_df[col] = 0.0\n\n    submission_df = submission_df[sample_sub.columns]\n\n    submission_df = submission_df.reset_index()\n    \n    return submission_df\n\n\ndef smooth_submission(submission_path):\n        \"\"\"\n        Post-process the submission CSV by smoothing predictions to enforce temporal consistency.\n        \n        For each soundscape (grouped by the file name part of 'row_id'), each row's predictions\n        are averaged with those of its neighbors using defined weights.\n        \n        :param submission_path: Path to the submission CSV file.\n        \"\"\"\n        print(\"Smoothing submission predictions...\")\n        sub = pd.read_csv(submission_path)\n        cols = sub.columns[1:]\n        # Extract group names by splitting row_id on the last underscore\n        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n        unique_groups = np.unique(groups)\n        \n        for group in unique_groups:\n            # Get indices for the current group\n            idx = np.where(groups == group)[0]\n            sub_group = sub.iloc[idx].copy()\n            predictions = sub_group[cols].values\n            new_predictions = predictions.copy()\n            \n            if predictions.shape[0] > 1:\n                # Smooth the predictions using neighboring segments\n                new_predictions[0] = (predictions[0] * 0.7) + (predictions[1] * 0.3)\n                new_predictions[-1] = (predictions[-1] * 0.7) + (predictions[-2] * 0.3)\n                for i in range(1, predictions.shape[0]-1):\n                    new_predictions[i] = (predictions[i-1] * 0.25) + (predictions[i] * 0.5) + (predictions[i+1] * 0.25)\n            # Replace the smoothed values in the submission dataframe\n            sub.iloc[idx, 1:] = new_predictions\n        \n        sub.to_csv(submission_path, index=False)\n        print(f\"Smoothed submission saved to {submission_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    print(\"Starting BirdCLEF-2025 inference...\")\n\n    models = load_models(cfg, num_classes)\n    \n    if not models:\n        print(\"No models found! Please check model paths.\")\n        return\n    \n    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n    start_time = time.time()\n    row_ids, predictions = run_inference(cfg, models, species_ids)\n    end_time = time.time()\n    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n\n    submission_path = 'submission.csv'\n    submission_df.to_csv(submission_path, index=False)\n    print(f\"Submission saved to {submission_path}\")\n\n    smooth_submission(submission_path)\n    \n\n    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}