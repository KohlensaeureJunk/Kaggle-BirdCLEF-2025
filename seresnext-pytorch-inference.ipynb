{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BirdCLEF 2025 Inference Notebook**\n",
    "This notebook runs inference on BirdCLEF 2025 test soundscapes and generates a submission file. It supports both single model inference and ensemble inference with multiple models. You can find the pre-processing and training processes in the following notebooks:\n",
    "\n",
    "- [Transforming Audio-to-Mel Spec. | BirdCLEF'25](https://www.kaggle.com/code/kadircandrisolu/transforming-audio-to-mel-spec-birdclef-25)  \n",
    "- [EfficientNet B0 Pytorch [Train] | BirdCLEF'25](https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-train-birdclef-25)\n",
    "\n",
    "**Features**\n",
    "- Audio Preprocessing\n",
    "- Test-Time Augmentation (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.150784Z",
     "iopub.status.busy": "2025-05-11T14:30:43.150420Z",
     "iopub.status.idle": "2025-05-11T14:30:43.156280Z",
     "shell.execute_reply": "2025-05-11T14:30:43.155331Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.150759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import torchaudio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.158125Z",
     "iopub.status.busy": "2025-05-11T14:30:43.157782Z",
     "iopub.status.idle": "2025-05-11T14:30:43.172373Z",
     "shell.execute_reply": "2025-05-11T14:30:43.171536Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.158105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    " \n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_path = '/kaggle/input/birdclef25-efficientnet-pseudlabled/pytorch/primary-only-thr-0.9/3'  \n",
    "    \n",
    "    # Audio parameters\n",
    "    FS = 32000  \n",
    "    WINDOW_SIZE = 5  \n",
    "    TARGET_DURATION = 5.0\n",
    "    \n",
    "    # Mel spectrogram parameters\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 14000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "    normal = 255\n",
    "    \n",
    "    # Model architecture - must match training notebook\n",
    "    model_name = 'seresnext26t_32x4d'  # Match training notebook default\n",
    "    in_channels = 1\n",
    "    device = 'cpu'\n",
    "    pretrained = False\n",
    "    dropout_rate = 0.2\n",
    "    drop_path_rate = 0.2\n",
    "    \n",
    "    # Inference parameters\n",
    "    batch_size = 16\n",
    "    use_tta = False  \n",
    "    tta_count = 3\n",
    "    threshold = 0.5\n",
    "    \n",
    "    use_specific_folds = False  # If False, use all found models\n",
    "    folds = [0, 1]  # Used only if use_specific_folds is True\n",
    "    \n",
    "    debug = False\n",
    "    debug_count = 3\n",
    "    \n",
    "    # Debug option for state dict loading\n",
    "    debug_state_dict = False  # Set to True to print missing keys\n",
    "\n",
    "cfg = CFG()\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.173714Z",
     "iopub.status.busy": "2025-05-11T14:30:43.173285Z",
     "iopub.status.idle": "2025-05-11T14:30:43.197604Z",
     "shell.execute_reply": "2025-05-11T14:30:43.196836Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.173689Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.199365Z",
     "iopub.status.busy": "2025-05-11T14:30:43.199125Z",
     "iopub.status.idle": "2025-05-11T14:30:43.210433Z",
     "shell.execute_reply": "2025-05-11T14:30:43.209662Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.199347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        super().__init__()\n",
    "        self.cfg = {attr: getattr(cfg, attr) for attr in dir(cfg) if not attr.startswith('__') and not callable(getattr(cfg, attr))}\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Support for different model architectures - match training notebook\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=cfg.dropout_rate,\n",
    "            drop_path_rate=cfg.drop_path_rate\n",
    "        )\n",
    "        self.bn0 = nn.BatchNorm2d(self.cfg['TARGET_SHAPE'][0])\n",
    "        layers = list(self.backbone.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        # Extract feature dimension based on model type\n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'convnext' in cfg.model_name:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        elif 'res' in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "        # Change activation to linear to get raw logits\n",
    "        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"linear\")\n",
    "\n",
    "        # IMPORTANT: These internal mel spectrogram transforms exist in the model but are NOT used\n",
    "        # during inference. Instead, we use the external preprocessing from transforming-training-audio\n",
    "        # notebook to ensure consistency with training data preprocessing.\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.cfg['FS'],\n",
    "            hop_length=self.cfg['HOP_LENGTH'],\n",
    "            n_mels=self.cfg['N_MELS'],\n",
    "            f_min=self.cfg['FMIN'],\n",
    "            f_max=self.cfg['FMAX'],\n",
    "            n_fft=self.cfg['N_FFT'],\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        if self.cfg['device'] == \"cuda\":\n",
    "            self.melspec_transform = self.melspec_transform.cuda()\n",
    "        else:\n",
    "            self.melspec_transform = self.melspec_transform.cpu()\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80\n",
    "        )\n",
    "\n",
    "    def extract_feature(self,x):\n",
    "        x = x.permute((0, 1, 3, 2))\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        x = x.transpose(2, 3)\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        \n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x, frames_num\n",
    "        \n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_to_spec(self, audio):\n",
    "        audio = audio.float()\n",
    "        \n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "\n",
    "        if self.cfg['normal'] == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif self.cfg['normal'] == 255:\n",
    "            spec = spec / 255\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return spec\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a mel spectrogram that was preprocessed externally using the same approach\n",
    "        # as the transforming-training-audio notebook (NOT using self.transform_to_spec)\n",
    "        x, frames_num = self.extract_feature(x)\n",
    "        \n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        # Return raw logits (no sigmoid/logit conversion)\n",
    "        return clipwise_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.232374Z",
     "iopub.status.busy": "2025-05-11T14:30:43.232063Z",
     "iopub.status.idle": "2025-05-11T14:30:43.251439Z",
     "shell.execute_reply": "2025-05-11T14:30:43.250634Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.232349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"\n",
    "    Convert audio data to mel spectrogram using the EXACT same approach as the \n",
    "    transforming-training-audio notebook. This ensures consistency between \n",
    "    training and inference preprocessing.\n",
    "    \"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Use the EXACT normalization from transforming-training-audio notebook\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"\n",
    "    Process audio segment to get mel spectrogram using the EXACT same approach as the \n",
    "    transforming-training-audio notebook. This function converts raw audio to the \n",
    "    mel spectrogram format that the model expects.\n",
    "    \"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data, \n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "                          mode='constant')\n",
    "    \n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    return mel_spec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.252842Z",
     "iopub.status.busy": "2025-05-11T14:30:43.252456Z",
     "iopub.status.idle": "2025-05-11T14:30:43.270181Z",
     "shell.execute_reply": "2025-05-11T14:30:43.269282Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.252816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "    \n",
    "    model_dir = Path(cfg.model_path)\n",
    "    \n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    model_files = find_model_files(cfg)\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    if cfg.use_specific_folds:\n",
    "        filtered_files = []\n",
    "        for fold in cfg.folds:\n",
    "            fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "            filtered_files.extend(fold_files)\n",
    "        model_files = filtered_files\n",
    "        print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n",
    "    \n",
    "    for model_path in model_files:\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=cfg.device)\n",
    "            \n",
    "            model = BirdCLEFModel(cfg, num_classes)\n",
    "            \n",
    "            # Handle different state dict configurations\n",
    "            state_dict = None\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['model_state_dict']\n",
    "            elif 'state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['state_dict']\n",
    "            else:\n",
    "                # Assume the checkpoint itself is the state dict\n",
    "                state_dict = checkpoint\n",
    "            \n",
    "            # Handle DataParallel wrapped state dict (remove 'module.' prefix)\n",
    "            if any(k.startswith('module.') for k in state_dict.keys()):\n",
    "                state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "            \n",
    "            # Check for missing keys before loading\n",
    "            model_dict = model.state_dict()\n",
    "            missing_keys = [k for k in model_dict.keys() if k not in state_dict]\n",
    "            unexpected_keys = [k for k in state_dict.keys() if k not in model_dict]\n",
    "            \n",
    "            if missing_keys and cfg.debug_state_dict:\n",
    "                print(f\"Missing keys: {missing_keys}\")\n",
    "            if unexpected_keys and cfg.debug_state_dict:\n",
    "                print(f\"Unexpected keys: {unexpected_keys}\")\n",
    "            \n",
    "            # Try to partially load if there are missing or unexpected keys\n",
    "            if missing_keys or unexpected_keys:\n",
    "                print(f\"Warning: {len(missing_keys)} missing keys, {len(unexpected_keys)} unexpected keys\")\n",
    "                # Filter state_dict to only include keys that are in the model\n",
    "                filtered_state_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "                model_dict.update(filtered_state_dict)\n",
    "                model.load_state_dict(model_dict, strict=False)\n",
    "                print(\"Loaded model with partial state dict\")\n",
    "            else:\n",
    "                model.load_state_dict(state_dict)\n",
    "                \n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            \n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"\n",
    "    Process a single audio file and predict species presence for each 5-second segment.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads raw audio from the file\n",
    "    2. Converts audio to mel spectrograms using the transforming-training-audio approach\n",
    "    3. Feeds the spectrograms to the model for prediction\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        # Load raw audio\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        \n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        \n",
    "        for segment_idx in range(total_segments):\n",
    "            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "            segment_audio = audio_data[start_sample:end_sample]\n",
    "            \n",
    "            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "            row_ids.append(row_id)\n",
    "\n",
    "            if cfg.use_tta:\n",
    "                all_preds = []\n",
    "                \n",
    "                for tta_idx in range(cfg.tta_count):\n",
    "                    # Convert audio segment to mel spectrogram using transforming-training-audio approach\n",
    "                    mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                    mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "\n",
    "                    mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                    mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "                    if len(models) == 1:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = models[0](mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            all_preds.append(probs)\n",
    "                    else:\n",
    "                        segment_preds = []\n",
    "                        for model in models:\n",
    "                            with torch.no_grad():\n",
    "                                outputs = model(mel_spec)\n",
    "                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                                segment_preds.append(probs)\n",
    "                        \n",
    "                        avg_preds = np.mean(segment_preds, axis=0)\n",
    "                        all_preds.append(avg_preds)\n",
    "\n",
    "                final_preds = np.mean(all_preds, axis=0)\n",
    "            else:\n",
    "                # Convert audio segment to mel spectrogram using transforming-training-audio approach\n",
    "                mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                \n",
    "                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                mel_spec = mel_spec.to(cfg.device)\n",
    "                \n",
    "                if len(models) == 1:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = models[0](mel_spec)\n",
    "                        final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    segment_preds = []\n",
    "                    for model in models:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            segment_preds.append(probs)\n",
    "\n",
    "                    final_preds = np.mean(segment_preds, axis=0)\n",
    "                    \n",
    "            predictions.append(final_preds)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.271930Z",
     "iopub.status.busy": "2025-05-11T14:30:43.271306Z",
     "iopub.status.idle": "2025-05-11T14:30:43.290954Z",
     "shell.execute_reply": "2025-05-11T14:30:43.290094Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.271884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_tta(spec, tta_idx):\n",
    "    \"\"\"Apply test-time augmentation\"\"\"\n",
    "    if tta_idx == 0:\n",
    "        # Original spectrogram\n",
    "        return spec\n",
    "    elif tta_idx == 1:\n",
    "        # Time shift (horizontal flip)\n",
    "        return np.flip(spec, axis=1)\n",
    "    elif tta_idx == 2:\n",
    "        # Frequency shift (vertical flip)\n",
    "        return np.flip(spec, axis=0)\n",
    "    else:\n",
    "        return spec\n",
    "\n",
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.293370Z",
     "iopub.status.busy": "2025-05-11T14:30:43.293138Z",
     "iopub.status.idle": "2025-05-11T14:30:43.310018Z",
     "shell.execute_reply": "2025-05-11T14:30:43.309271Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.293353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference...\")\n",
    "    print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "    print(f\"Using model architecture: {cfg.model_name}\")\n",
    "\n",
    "    models = load_models(cfg, num_classes)\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_model_compatibility():\n",
    "    \"\"\"\n",
    "    Function to debug model compatibility issues\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Debugging Model Compatibility ===\")\n",
    "    \n",
    "    print(f\"Creating a test model with the current configuration...\")\n",
    "    test_model = BirdCLEFModel(cfg, num_classes)\n",
    "    test_state_dict = test_model.state_dict()\n",
    "    \n",
    "    print(f\"Model has {len(test_state_dict)} parameters\")\n",
    "    \n",
    "    # Display some parameter shapes to help debug\n",
    "    print(\"\\nSample parameter shapes from current model:\")\n",
    "    for i, (name, param) in enumerate(test_state_dict.items()):\n",
    "        if i < 5 or i > len(test_state_dict) - 5:  # First 5 and last 5 params\n",
    "            print(f\"{name}: {param.shape}\")\n",
    "        if i == 5 and len(test_state_dict) > 10:\n",
    "            print(\"...\")\n",
    "            \n",
    "    # Try to load sample model\n",
    "    model_files = find_model_files(cfg)\n",
    "    if model_files:\n",
    "        sample_file = model_files[0]\n",
    "        print(f\"\\nAttempting to load a sample model: {sample_file}\")\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(sample_file, map_location=cfg.device)\n",
    "            \n",
    "            # Check what's in the checkpoint\n",
    "            if isinstance(checkpoint, dict):\n",
    "                print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "                \n",
    "                # Look at state dict keys\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    state_dict = checkpoint['model_state_dict']\n",
    "                elif 'state_dict' in checkpoint:\n",
    "                    state_dict = checkpoint['state_dict']\n",
    "                else:\n",
    "                    state_dict = None\n",
    "                    print(\"No state dict found in checkpoint\")\n",
    "                \n",
    "                if state_dict:\n",
    "                    print(f\"State dict has {len(state_dict)} parameters\")\n",
    "                    \n",
    "                    # Count mismatches\n",
    "                    missing = [k for k in test_state_dict.keys() if k not in state_dict]\n",
    "                    unexpected = [k for k in state_dict.keys() if k not in test_state_dict]\n",
    "                    \n",
    "                    print(f\"Missing keys: {len(missing)}\")\n",
    "                    print(f\"Unexpected keys: {len(unexpected)}\")\n",
    "                    \n",
    "                    if missing:\n",
    "                        print(\"\\nSample missing keys:\")\n",
    "                        for k in missing[:5]:\n",
    "                            print(f\"  {k}\")\n",
    "                    \n",
    "                    if unexpected:\n",
    "                        print(\"\\nSample unexpected keys:\")\n",
    "                        for k in unexpected[:5]:\n",
    "                            print(f\"  {k}\")\n",
    "                        \n",
    "                    # Try to map keys\n",
    "                    if unexpected and missing:\n",
    "                        print(\"\\nAttempting to map keys...\")\n",
    "                        successful_maps = 0\n",
    "                        for unexp_key in unexpected[:10]:\n",
    "                            for miss_key in missing:\n",
    "                                # Simple heuristic: check if the key ends with the same parameter name\n",
    "                                if unexp_key.split('.')[-1] == miss_key.split('.')[-1]:\n",
    "                                    print(f\"Possible mapping: {unexp_key} -> {miss_key}\")\n",
    "                                    successful_maps += 1\n",
    "                                    break\n",
    "                        \n",
    "                        if successful_maps == 0:\n",
    "                            print(\"No obvious mappings found. Model architectures may be different.\")\n",
    "            else:\n",
    "                print(\"Checkpoint is not a dictionary - it may be a direct state dict\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n=== End of Compatibility Debugging ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T14:30:43.311088Z",
     "iopub.status.busy": "2025-05-11T14:30:43.310789Z",
     "iopub.status.idle": "2025-05-11T14:30:43.406526Z",
     "shell.execute_reply": "2025-05-11T14:30:43.405673Z",
     "shell.execute_reply.started": "2025-05-11T14:30:43.311008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "TTA enabled: False (variations: 0)\n",
      "Found a total of 1 model files.\n",
      "Loading model: /kaggle/input/birdclef25-efficientnet-pseudlabled/pytorch/primary-only-thr-0.9/3/model_20250501_131702_efficientnet_b0_fold0.pth\n",
      "Error loading model /kaggle/input/birdclef25-efficientnet-pseudlabled/pytorch/primary-only-thr-0.9/3/model_20250501_131702_efficientnet_b0_fold0.pth: BirdCLEFModel.__init__() takes 2 positional arguments but 3 were given\n",
      "No models found! Please check model paths.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Debug model compatibility if needed\n",
    "    if cfg.debug_state_dict:\n",
    "        debug_model_compatibility()\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 6891568,
     "sourceId": 11060723,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 234673081,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 320888,
     "modelInstanceId": 300331,
     "sourceId": 361214,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 320888,
     "modelInstanceId": 300331,
     "sourceId": 366989,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 324738,
     "modelInstanceId": 304257,
     "sourceId": 367012,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 320888,
     "modelInstanceId": 300331,
     "sourceId": 367914,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 320888,
     "modelInstanceId": 300331,
     "sourceId": 387055,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
