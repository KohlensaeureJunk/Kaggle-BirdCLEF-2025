{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11948763,"sourceType":"datasetVersion","datasetId":7130272},{"sourceId":426264,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":347484,"modelId":365373},{"sourceId":427449,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":348452,"modelId":365373}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Inference Notebook for final submission\nCombined data filtering, pseudo-labels and spectrogram augmentations in one final model. \\\nUsing more powerful [SE-ResNeXt](https://paperswithcode.com/model/seresnext?variant=seresnext50-32x4d) model. \\\nFor more informmation, see training notebook:\n- [MLiP Group 25 BirdCLEF 2025 Training Notebook](https://www.kaggle.com/code/maxgewald/mlip-25-birdclef2025-training) \n\nThis notebook is a modified version of:\n- [Bird2025 | Single SED Model Inference [LB 0.857]](www.kaggle.com/code/maxgewald/mlip-birdclef-seresnext-inference/edit)","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport warnings\nimport logging\nimport time\nimport math\nimport cv2\nfrom pathlib import Path\nimport joblib\n\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport soundfile as sf\nfrom soundfile import SoundFile \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast, GradScaler\nimport timm\nfrom tqdm.auto import tqdm\nfrom glob import glob\nimport torchaudio\nimport random\nimport itertools\nfrom typing import Union\n\nimport concurrent.futures\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.ERROR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:09:24.186956Z","iopub.execute_input":"2025-06-09T01:09:24.190750Z","iopub.status.idle":"2025-06-09T01:09:35.186315Z","shell.execute_reply.started":"2025-06-09T01:09:24.190647Z","shell.execute_reply":"2025-06-09T01:09:35.185101Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class CFG:\n    \n    seed = 42\n    print_freq = 100\n    num_workers = 4\n    batch_size=4\n\n    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n    model_paths = ['/kaggle/input/birdclef-2025-mlip-submission/pytorch/label-filtering-pseudo-labels-5-folds/1/',\n                  '/kaggle/input/birdclef-2025-mlip-submission/pytorch/seresnext-full-training/1']\n \n    pretrained = False\n    in_channels = 1\n    \n    # Mel spectrogram parameters\n    n_fft = 1024\n    hop_length = 512\n    n_mels = 128\n    f_min = 50\n    f_max = 14000\n    target_shape = (256,256)\n\n    projection_dim = 512\n    projection_dropout = 0.0\n    \n    SR = 32000\n    target_duration = 5\n    infer_duration = 5\n    train_duration = 5\n    \n    device = 'cpu'\n\ncfg = CFG()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:09:35.187910Z","iopub.execute_input":"2025-06-09T01:09:35.188493Z","iopub.status.idle":"2025-06-09T01:09:35.195703Z","shell.execute_reply.started":"2025-06-09T01:09:35.188463Z","shell.execute_reply":"2025-06-09T01:09:35.194679Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print(f\"Using device: {cfg.device}\")\nprint(f\"Loading taxonomy data...\")\ntaxonomy_df = pd.read_csv(cfg.taxonomy_csv)\nspecies_ids = taxonomy_df['primary_label'].tolist()\nnum_classes = len(species_ids)\nprint(f\"Number of classes: {num_classes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:09:35.197034Z","iopub.execute_input":"2025-06-09T01:09:35.197305Z","iopub.status.idle":"2025-06-09T01:09:35.243097Z","shell.execute_reply.started":"2025-06-09T01:09:35.197277Z","shell.execute_reply":"2025-06-09T01:09:35.241835Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nLoading taxonomy data...\nNumber of classes: 206\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def set_seed(seed=42):\n    \"\"\"\n    Set seed for reproducibility\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(cfg.seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:09:35.244213Z","iopub.execute_input":"2025-06-09T01:09:35.244608Z","iopub.status.idle":"2025-06-09T01:09:35.255944Z","shell.execute_reply.started":"2025-06-09T01:09:35.244577Z","shell.execute_reply":"2025-06-09T01:09:35.254664Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class AttBlockV2(nn.Module):\n    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n        super().__init__()\n\n        self.activation = activation\n        self.att = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True,\n        )\n        self.cla = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True,\n        )\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n\n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x, norm_att, cla\n\n    def nonlinear_transform(self, x):\n        if self.activation == \"linear\":\n            return x\n        elif self.activation == \"sigmoid\":\n            return torch.sigmoid(x)\n\n\ndef init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.0)\n\ndef init_bn(bn):\n    bn.bias.data.fill_(0.0)\n    bn.weight.data.fill_(1.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:09:35.259639Z","iopub.execute_input":"2025-06-09T01:09:35.259964Z","iopub.status.idle":"2025-06-09T01:09:35.274160Z","shell.execute_reply.started":"2025-06-09T01:09:35.259939Z","shell.execute_reply":"2025-06-09T01:09:35.272854Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class SeResNextModel(nn.Module):\n    def __init__(self, cfg, name):\n        super().__init__()\n        \n        taxonomy_df = pd.read_csv('/kaggle/input/birdclef-2025/taxonomy.csv')\n        self.num_classes = len(taxonomy_df)\n\n        self.bn0 = nn.BatchNorm2d(cfg.target_shape[0])\n        \n        self.backbone = timm.create_model(\n            name,\n            pretrained=False,\n            in_chans=cfg.in_channels,\n            drop_rate=0.0,\n            drop_path_rate=0.0,\n        )\n\n        layers = list(self.backbone.children())[:-2]\n        self.encoder = nn.Sequential(*layers)\n\n        backbone_out = self.backbone.fc.in_features\n        self.backbone.fc = nn.Identity()\n        \n        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n\n        # this is not used, but needed for compatibility\n        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=cfg.SR,\n            hop_length=cfg.hop_length,\n            n_mels=cfg.n_mels,\n            f_min=cfg.f_min,\n            f_max=cfg.f_max,\n            n_fft=cfg.n_fft,\n            pad_mode=\"constant\",\n            norm=\"slaney\",\n            onesided=True,\n            mel_scale=\"htk\",\n        )\n\n\n    def extract_feature(self,x):\n        x = x.permute((0, 1, 3, 2))\n        frames_num = x.shape[2]\n        \n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n        \n        x = x.transpose(2, 3)\n        # (batch_size, channels, freq, frames)\n        x = self.encoder(x)\n        \n        # (batch_size, channels, frames)\n        x = torch.mean(x, dim=2)\n        \n        # channel smoothing\n        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x = x1 + x2\n        \n        x = F.dropout(x, p=0.5, training=self.training)\n        x = x.transpose(1, 2)\n        x = F.relu_(self.fc1(x))\n        x = x.transpose(1, 2)\n        x = F.dropout(x, p=0.5, training=self.training)\n        return x, frames_num\n\n    def forward(self, x):\n\n        x, frames_num = self.extract_feature(x)\n        \n        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n        segmentwise_output = segmentwise_output.transpose(1, 2)\n\n        return torch.logit(clipwise_output)\n\n    def infer(self, x, tta_delta=2):\n\n        x,_ = self.extract_feature(x)\n        time_att = torch.tanh(self.att_block.att(x))\n        feat_time = x.size(-1)\n        start = (\n            feat_time / 2 - feat_time * (cfg.infer_duration / cfg.train_duration) / 2\n        )\n        end = start + feat_time * (cfg.infer_duration / cfg.train_duration)\n        start = int(start)\n        end = int(end)\n        pred = self.attention_infer(start,end,x,time_att)\n\n        start_minus = max(0, start-tta_delta)\n        end_minus=end-tta_delta\n        pred_minus = self.attention_infer(start_minus,end_minus,x,time_att)\n\n        start_plus = start+tta_delta\n        end_plus=min(feat_time, end+tta_delta)\n        pred_plus = self.attention_infer(start_plus,end_plus,x,time_att)\n\n        pred = 0.5*pred + 0.25*pred_minus + 0.25*pred_plus\n        return pred\n        \n    def attention_infer(self,start,end,x,time_att):\n        feat = x[:, :, start:end]\n        # att = torch.softmax(time_att[:, :, start:end], dim=-1)\n        #             print(feat_time, start, end)\n        #             print(att_a.sum(), att.sum(), time_att.shape)\n        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n        framewise_pred_max = framewise_pred.max(dim=2)[0]\n        # clipwise_output = torch.sum(framewise_pred * att, dim=-1)\n        #logits = torch.sum(\n        #    self.att_block.cla(feat) * att,\n        #    dim=-1,\n        #)\n\n        # return clipwise_output\n        return framewise_pred_max","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:09:35.275070Z","iopub.execute_input":"2025-06-09T01:09:35.275354Z","iopub.status.idle":"2025-06-09T01:09:35.298085Z","shell.execute_reply.started":"2025-06-09T01:09:35.275331Z","shell.execute_reply":"2025-06-09T01:09:35.297003Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class EffnetModel(nn.Module):\n    def __init__(self, cfg, name):\n        super().__init__()\n        self.cfg = cfg\n        self.backbone = timm.create_model(\n            name,\n            pretrained=cfg.pretrained,\n            in_chans=cfg.in_channels,\n        )\n        backbone_out = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        \n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.feat_dim = backbone_out\n        if cfg.projection_dim > 0:\n            self.projection = nn.Sequential(\n                nn.Linear(backbone_out, cfg.projection_dim),\n                nn.BatchNorm1d(cfg.projection_dim),\n                nn.ReLU(inplace=True),\n                nn.Dropout(cfg.projection_dropout),\n                nn.Linear(cfg.projection_dim, num_classes)\n            )\n            self.classifier = self.projection\n        else:\n            self.classifier = nn.Linear(backbone_out, num_classes)\n\n        \n    def forward(self, x, targets=None):\n\n        features = self.backbone(x)\n            \n        if len(features.shape) == 4:\n            features = self.pooling(features)\n            features = features.view(features.size(0), -1)\n        \n        logits = self.classifier(features)\n        return logits\n\n    def infer(self, x):\n        return self.forward(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:09:35.299559Z","iopub.execute_input":"2025-06-09T01:09:35.299907Z","iopub.status.idle":"2025-06-09T01:09:35.329293Z","shell.execute_reply.started":"2025-06-09T01:09:35.299880Z","shell.execute_reply":"2025-06-09T01:09:35.328336Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def transform_to_spec(audio_data):\n\n    if np.isnan(audio_data).any():\n        mean_signal = np.nanmean(audio_data)\n        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n\n    mel_spec = librosa.feature.melspectrogram(\n        y=audio_data,\n        sr=cfg.SR,\n        n_fft=cfg.n_fft,\n        hop_length=cfg.hop_length,\n        n_mels=cfg.n_mels,\n        fmin=cfg.f_min,\n        fmax=cfg.f_max,\n        power=2.0\n    )\n\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    mel_min, mel_max = mel_spec_db.min(), mel_spec_db.max()\n    mel_spec_norm = (mel_spec_db - mel_min) / (mel_max - mel_min + 1e-8)\n    \n    if mel_spec_norm.shape != cfg.target_shape:\n        mel_spec_norm = cv2.resize(mel_spec_norm, cfg.target_shape, interpolation=cv2.INTER_LINEAR)\n    \n    return torch.tensor(mel_spec_norm, dtype=torch.float32).unsqueeze(0).unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:09:35.330210Z","iopub.execute_input":"2025-06-09T01:09:35.330525Z","iopub.status.idle":"2025-06-09T01:09:35.356306Z","shell.execute_reply.started":"2025-06-09T01:09:35.330493Z","shell.execute_reply":"2025-06-09T01:09:35.355310Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def load_sample(path, cfg):\n    audio, _ = sf.read(path, dtype=\"float32\")\n    audio_length = cfg.SR * cfg.target_duration\n    step = audio_length\n    segments = []\n    \n    # Pre-calculate all segments\n    for i in range(audio_length, len(audio) + step, step):\n        start = max(0, i - audio_length)\n        end = start + audio_length\n        if end <= len(audio):\n            segments.append((start, end))\n    \n    # Pad audio once\n    padded_audio = np.pad(audio, (len(audio), len(audio)), mode='wrap')\n    \n    audios = []\n    train_length = int(cfg.SR * cfg.train_duration)\n    target_length = int(cfg.SR * cfg.target_duration)\n    pad_length = (train_length - target_length) // 2\n    \n    for i, (start, end) in enumerate(segments):\n        center = len(audio) + (start + end) // 2\n        segment_start = center - train_length // 2\n        segment_end = segment_start + train_length\n        \n        y = padded_audio[segment_start:segment_end].astype(np.float32)\n        \n        # Apply padding only at boundaries\n        if i == 0:\n            y[:pad_length] = 0\n        elif i == len(segments) - 1:\n            y[-pad_length:] = 0\n            \n        audios.append(y)\n    \n    return audios\n\ndef sigmoid(x):\n    s = 1 / (1 + np.exp(-x))\n    return s","metadata":{"execution":{"iopub.status.busy":"2025-06-09T01:09:35.357348Z","iopub.execute_input":"2025-06-09T01:09:35.357682Z","iopub.status.idle":"2025-06-09T01:09:35.384078Z","shell.execute_reply.started":"2025-06-09T01:09:35.357653Z","shell.execute_reply":"2025-06-09T01:09:35.382724Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def find_model_files(cfg):\n    \"\"\"\n    Find all .pth model files in the specified model directory\n    \"\"\"\n    model_files = []\n    \n    model_dirs = [Path(path) for path in cfg.model_paths]\n\n    for model_dir in model_dirs:\n        for path in model_dir.glob('**/*.pth'):\n            model_files.append(str(path))\n    \n    return model_files\n\ndef load_models(cfg, num_classes):\n    \"\"\"\n    Load all found model files and prepare them for ensemble\n    \"\"\"\n    models = []\n    \n    model_files = find_model_files(cfg)\n    \n    if not model_files:\n        print(f\"Warning: No model files found under {cfg.model_path}!\")\n        return models\n    \n    print(f\"Found a total of {len(model_files)} model files.\")\n    \n    for i, model_path in enumerate(model_files):\n        print(f\"Loading model: {model_path}\")\n        checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=False)\n    \n        if \"efficientnet_b3\" in model_path:\n            model = EffnetModel(cfg, \"efficientnet_b3\")\n        elif \"resnext\" in model_path:\n            model = SeResNextModel(cfg, \"seresnext26t_32x4d\")\n            \n        state_dict = checkpoint['model_state_dict']\n        if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n            state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n            \n        model.load_state_dict(state_dict)\n        model = model.to(cfg.device)\n        model.eval()\n        model.zero_grad()\n        model = model.half().float()\n        models.append(model)\n    torch.set_num_threads(4)\n\n    \n    return models\n\ndef predict_on_spectrogram(audio_paths, models, cfg):\n    \"\"\"Process multiple audio files in batches\"\"\"\n    all_row_ids = []\n    all_predictions = []\n    \n    for i in range(0, len(audio_paths), cfg.batch_size):\n        batch_paths = audio_paths[i:i+cfg.batch_size]\n        batch_specs = []\n        batch_row_ids = []\n        \n        # Load and preprocess batch\n        for audio_path in batch_paths:\n            audio_path = str(audio_path)\n            soundscape_id = Path(audio_path).stem\n            audio_data = load_sample(audio_path, cfg)\n            \n            for segment_idx, audio_input in enumerate(audio_data):\n                end_time_sec = (segment_idx + 1) * cfg.target_duration\n                row_id = f\"{soundscape_id}_{end_time_sec}\"\n                batch_row_ids.append(row_id)\n                \n                spec = transform_to_spec(audio_input)\n                batch_specs.append(spec)\n        \n        # Process batch\n        if batch_specs:\n            batch_tensor = torch.cat(batch_specs, dim=0)\n            \n            with torch.no_grad():\n                if len(models) == 1:\n                    outputs = models[0].infer(batch_tensor)\n                    batch_preds = outputs.cpu().numpy()\n                else:\n                    batch_ensemble_preds = []\n                    for model in models:\n                        outputs = model.infer(batch_tensor)\n                        batch_ensemble_preds.append(outputs.cpu().numpy())\n                    batch_preds = np.mean(batch_ensemble_preds, axis=0)\n            \n            all_row_ids.extend(batch_row_ids)\n            all_predictions.extend(batch_preds)\n    \n    return all_row_ids, all_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:09:35.385241Z","iopub.execute_input":"2025-06-09T01:09:35.385553Z","iopub.status.idle":"2025-06-09T01:09:35.411299Z","shell.execute_reply.started":"2025-06-09T01:09:35.385531Z","shell.execute_reply":"2025-06-09T01:09:35.410065Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def run_inference(cfg, models, species_ids):\n    \"\"\"Run inference on all test soundscapes\"\"\"\n    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n    if len(test_files) == 0:\n        test_files = sorted(glob(str(Path('/kaggle/input/birdclef-2025/train_soundscapes') / '*.ogg')))[:50]\n    \n    print(f\"Found {len(test_files)} test soundscapes\")\n    \n    all_row_ids = []\n    all_predictions = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cfg.num_workers) as executor:\n        results = list(\n            executor.map(\n                predict_on_spectrogram,\n                test_files,\n                itertools.repeat(models),\n                itertools.repeat(cfg),\n            )\n        )\n\n    for rids, preds in results:\n        all_row_ids.extend(rids)\n        all_predictions.extend(preds)\n    \n    return all_row_ids, all_predictions\n\ndef create_submission(row_ids, predictions, species_ids, cfg):\n    \"\"\"Create submission dataframe\"\"\"\n    print(\"Creating submission dataframe...\")\n\n    submission_dict = {'row_id': row_ids}\n    \n    for i, species in enumerate(species_ids):\n        submission_dict[species] = [pred[i] for pred in predictions]\n\n    submission_df = pd.DataFrame(submission_dict)\n\n    submission_df.set_index('row_id', inplace=True)\n\n    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n\n    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n    if missing_cols:\n        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n        for col in missing_cols:\n            submission_df[col] = 0.0\n\n    submission_df = submission_df[sample_sub.columns]\n\n    submission_df = submission_df.reset_index()\n    \n    return submission_df\n\n\ndef smooth_submission(submission_path):\n        \"\"\"\n        Post-process the submission CSV by smoothing predictions to enforce temporal consistency.\n        \n        For each soundscape (grouped by the file name part of 'row_id'), each row's predictions\n        are averaged with those of its neighbors using defined weights.\n        \n        :param submission_path: Path to the submission CSV file.\n        \"\"\"\n        print(\"Smoothing submission predictions...\")\n        sub = pd.read_csv(submission_path)\n        cols = sub.columns[1:]\n        # Extract group names by splitting row_id on the last underscore\n        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n        unique_groups = np.unique(groups)\n        \n        for group in unique_groups:\n            # Get indices for the current group\n            idx = np.where(groups == group)[0]\n            sub_group = sub.iloc[idx].copy()\n            predictions = sub_group[cols].values\n            new_predictions = predictions.copy()\n            \n            if predictions.shape[0] > 1:\n                # Smooth the predictions using neighboring segments\n                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n                for i in range(1, predictions.shape[0]-1):\n                    new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n            # Replace the smoothed values in the submission dataframe\n            sub.iloc[idx, 1:] = new_predictions\n        \n        sub.to_csv(submission_path, index=False)\n        print(f\"Smoothed submission saved to {submission_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:15:45.232897Z","iopub.execute_input":"2025-06-09T01:15:45.233398Z","iopub.status.idle":"2025-06-09T01:15:45.249082Z","shell.execute_reply.started":"2025-06-09T01:15:45.233369Z","shell.execute_reply":"2025-06-09T01:15:45.248109Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def main():\n    print(\"Starting BirdCLEF-2025 inference...\")\n\n    models = load_models(cfg, num_classes)\n    \n    if not models:\n        print(\"No models found! Please check model paths.\")\n        return\n    \n    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n    start_time = time.time()\n    row_ids, predictions = run_inference(cfg, models, species_ids)\n    end_time = time.time()\n    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n\n    submission_path = 'submission.csv'\n    submission_df.to_csv(submission_path, index=False)\n    print(f\"Submission saved to {submission_path}\")\n\n    smooth_submission(submission_path)\n    \n\n    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:10:09.321718Z","iopub.execute_input":"2025-06-09T01:10:09.322038Z","iopub.status.idle":"2025-06-09T01:10:09.329580Z","shell.execute_reply.started":"2025-06-09T01:10:09.322018Z","shell.execute_reply":"2025-06-09T01:10:09.328334Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:15:49.120841Z","iopub.execute_input":"2025-06-09T01:15:49.121181Z","iopub.status.idle":"2025-06-09T01:15:55.509365Z","shell.execute_reply.started":"2025-06-09T01:15:49.121156Z","shell.execute_reply":"2025-06-09T01:15:55.507834Z"}},"outputs":[{"name":"stdout","text":"Starting BirdCLEF-2025 inference...\nFound a total of 10 model files.\nLoading model: /kaggle/input/birdclef-2025-mlip-submission/pytorch/label-filtering-pseudo-labels-5-folds/1/model_20250605_192722_efficientnet_b3_fold1.pth\nLoading model: /kaggle/input/birdclef-2025-mlip-submission/pytorch/label-filtering-pseudo-labels-5-folds/1/model_20250605_192722_efficientnet_b3_fold0.pth\nLoading model: /kaggle/input/birdclef-2025-mlip-submission/pytorch/label-filtering-pseudo-labels-5-folds/1/model_20250605_192722_efficientnet_b3_fold2.pth\nLoading model: /kaggle/input/birdclef-2025-mlip-submission/pytorch/label-filtering-pseudo-labels-5-folds/1/model_20250605_192722_efficientnet_b3_fold4.pth\nLoading model: /kaggle/input/birdclef-2025-mlip-submission/pytorch/label-filtering-pseudo-labels-5-folds/1/model_20250605_192722_efficientnet_b3_fold3.pth\nLoading model: /kaggle/input/birdclef-2025-mlip-submission/pytorch/seresnext-full-training/1/model_20250607_002141_seresnext26t_32x4d_fold4.pth\nLoading model: /kaggle/input/birdclef-2025-mlip-submission/pytorch/seresnext-full-training/1/model_20250607_002141_seresnext26t_32x4d_fold3.pth\nLoading model: /kaggle/input/birdclef-2025-mlip-submission/pytorch/seresnext-full-training/1/model_20250607_002141_seresnext26t_32x4d_fold1.pth\nLoading model: /kaggle/input/birdclef-2025-mlip-submission/pytorch/seresnext-full-training/1/model_20250607_002141_seresnext26t_32x4d_fold2.pth\nLoading model: /kaggle/input/birdclef-2025-mlip-submission/pytorch/seresnext-full-training/1/model_20250607_002141_seresnext26t_32x4d_fold0.pth\nModel usage: Ensemble of 10 models\nFound 50 test soundscapes\nFalse\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_94/3832242952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_94/940001693.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrow_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecies_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecies_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_94/1035176296.py\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(cfg, models, species_ids)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'/'\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         results = list(\n\u001b[0m\u001b[1;32m     14\u001b[0m             executor.map(\n\u001b[1;32m     15\u001b[0m                 \u001b[0mpredict_on_spectrogram\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    617\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_94/2200405191.py\u001b[0m in \u001b[0;36mpredict_on_spectrogram\u001b[0;34m(audio_paths, models, cfg)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0maudio_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0msoundscape_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0maudio_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msegment_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_input\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_94/495156628.py\u001b[0m in \u001b[0;36mload_sample\u001b[0;34m(path, cfg)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0maudio_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_duration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/soundfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(file, frames, start, stop, dtype, always_2d, fill_value, out, samplerate, channels, format, subtype, endian, closefd)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \"\"\"\n\u001b[0;32m--> 305\u001b[0;31m     with SoundFile(file, 'r', samplerate, channels,\n\u001b[0m\u001b[1;32m    306\u001b[0m                    subtype, endian, format, closefd) as f:\n\u001b[1;32m    307\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[1;32m    688\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    689\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;31m# get the actual error code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLibsndfileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error opening {0!r}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0;31m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLibsndfileError\u001b[0m: Error opening '/': Format not recognised."],"ename":"LibsndfileError","evalue":"Error opening '/': Format not recognised.","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}